#!/usr/bin/env python
"""
Unified prediction script for both full fine-tuned and QLoRA checkpoints.

Supports two checkpoint types (auto-detected):
  - Full fine-tuned: Loads entire model from checkpoint (model.safetensors)
  - QLoRA: Loads base model from HuggingFace in 4-bit + LoRA adapters (adapter_model.safetensors)

Uses device_map='auto' to spread model across available GPUs for inference.

Reads config JSON generated by generate_predict_configs.py:
  - checkpoint_dir: full path to checkpoint (e.g. .../split_0/checkpoint-best)
  - output_dir: path to prediction results directory

Auto-detects whether test data has labels:
  - Labels present: computes metrics (accuracy, precision, recall, F1) after prediction
  - No labels: saves predictions only, writes empty all_results.json

Usage:
    # From model-specific directory:
    cd meta-llama_llama-3.1-8b-instruct/
    python ../predict.py pred_runs_.../split_0/checkpoint-best/multi_sentence_test/config.json
"""

import argparse
import json
import os
import sys

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoConfig,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    BitsAndBytesConfig,
)

LABEL_LIST = [
    "NOT", "Association", "Positive_Correlation", "Negative_Correlation",
    "Bind", "Cotreatment", "Comparison", "Drug_Interaction", "Conversion",
]
LABEL_DICT = {idx: val for idx, val in enumerate(LABEL_LIST)}


class SentenceDataset(Dataset):
    def __init__(self, sentences, labels, tokenizer, max_len):
        self.sentences = sentences
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, item):
        sentence = str(self.sentences[item])
        label = self.labels[item]
        encoding = self.tokenizer(
            sentence,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'sentence': sentence,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long),
        }


def transform_sentence(entry, transform_method, move_entities_to_start):
    entity_a = entry["entity_a"]
    entity_b = entry["entity_b"]
    sent = entry["text"]
    if sent == "":
        return ""

    all_poses = [(s, e, t, 'a') for s, e, t in entity_a] + \
                [(s, e, t, 'b') for s, e, t in entity_b]
    all_poses.sort(key=lambda i: i[0], reverse=True)

    for start, end, e_type, entity_id in all_poses:
        mention = entry["text"][start:end]
        if transform_method == "entity_mask":
            pre, post = "", ""
            ent = e_type
        elif transform_method == "entity_marker":
            if entity_id == 'a':
                pre, post = "[E1] ", " [/E1]"
            else:
                pre, post = "[E2] ", " [/E2]"
            ent = mention
        elif transform_method == "entity_marker_punkt":
            if entity_id == 'a':
                pre, post = "@ ", " @"
            else:
                pre, post = "# ", " #"
            ent = mention
        elif transform_method == "typed_entity_marker":
            pre, post = f"[{e_type}] ", f" [/{e_type}]"
            ent = mention
        elif transform_method == "typed_entity_marker_punct":
            if entity_id == 'a':
                pre, post = f"@ * {e_type} * ", " @"
            else:
                pre, post = f"# ^ {e_type} ^ ", " #"
            ent = mention
        elif transform_method == "subject_object_marker":
            if entity_id == 'a':
                pre, post = "[SUBJECT] ", " [/SUBJECT]"
            else:
                pre, post = "[OBJECT] ", " [/OBJECT]"
            ent = mention
        else:
            raise NotImplementedError(f"{transform_method} is not implemented!")

        replacement = pre + ent + post
        sent = sent[0:start] + replacement + sent[end:]

    if move_entities_to_start:
        sent = entry["text"][entity_a[0][0]:entity_a[0][1]] + ", " + \
               entry["text"][entity_b[0][0]:entity_b[0][1]] + ", " + sent

    return sent


def process_data(df, transform_method, move_entities_to_start, label_column_name="type", mode='train', use_prompt_format=False):
    if mode == 'train':
        label_to_id = {label: idx for idx, label in LABEL_DICT.items()}
        df["label"] = df[label_column_name].map(label_to_id)
    elif mode == 'test':
        df["label"] = 0

    # Extract entity names from first span BEFORE transform_sentence modifies text
    entity_a_names = []
    entity_b_names = []
    for _, entry in df.iterrows():
        ea = entry["entity_a"]
        eb = entry["entity_b"]
        raw_text = entry["text"]
        entity_a_names.append(raw_text[ea[0][0]:ea[0][1]])
        entity_b_names.append(raw_text[eb[0][0]:eb[0][1]])

    df["text"] = [
        transform_sentence(entry, transform_method, move_entities_to_start)
        for _, entry in df.iterrows()
    ]

    if use_prompt_format:
        relation_types_str = ", ".join(LABEL_LIST)
        prompts = []
        for idx, (_, row) in enumerate(df.iterrows()):
            prompt = (
                "You are a biomedical relation extraction model.\n"
                "Determine the relation between the subject and object from these types:\n"
                f"{relation_types_str}\n\n"
                f"Text: {row['text']}\n"
                f"Entity 1: {entity_a_names[idx]}\n"
                f"Entity 2: {entity_b_names[idx]}"
            )
            prompts.append(prompt)
        df["text"] = prompts

    return df


def get_dataframes(cmd_str):
    str_list = cmd_str.split(";")
    dataframes = [pd.read_json(path) for path in str_list]
    return pd.concat(dataframes, ignore_index=True)


def get_input_device(model):
    """Get the device where inputs should be sent (first layer's device).

    Traverses PeftModel wrapper if present to find the underlying hf_device_map.
    """
    # Traverse PeftModel -> base_model -> model to find hf_device_map
    current = model
    for attr in ('base_model', 'model'):
        if hasattr(current, 'hf_device_map'):
            first_device = next(iter(current.hf_device_map.values()))
            return torch.device(first_device)
        if hasattr(current, attr):
            current = getattr(current, attr)
    if hasattr(current, 'hf_device_map'):
        first_device = next(iter(current.hf_device_map.values()))
        return torch.device(first_device)
    return next(model.parameters()).device


@torch.no_grad()
def run_predictions(model, dataloader, input_device, desc="Predicting"):
    """Run inference on a DataLoader, return all logits."""
    from tqdm import tqdm
    model.eval()
    all_logits = []
    for batch in tqdm(dataloader, desc=desc, unit="batch"):
        input_ids = batch['input_ids'].to(input_device)
        attention_mask = batch['attention_mask'].to(input_device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        all_logits.append(outputs.logits.cpu().float().numpy())
    return np.concatenate(all_logits, axis=0)


def detect_checkpoint_type(checkpoint_path):
    """Auto-detect whether a checkpoint is QLoRA (adapter) or full fine-tuned.

    Returns:
        "qlora" if adapter_config.json is present
        "full_ft" if model.safetensors (or sharded) is present
        None if neither is found
    """
    adapter_config = os.path.join(checkpoint_path, "adapter_config.json")
    model_safetensors = os.path.join(checkpoint_path, "model.safetensors")
    model_index = os.path.join(checkpoint_path, "model.safetensors.index.json")

    if os.path.isfile(adapter_config):
        return "qlora"
    if os.path.isfile(model_safetensors) or os.path.isfile(model_index):
        return "full_ft"
    return None


def load_model_full_ft(checkpoint_path, tokenizer):
    """Load a full fine-tuned model from checkpoint."""
    print("Loading full fine-tuned model from checkpoint...")
    model_config = AutoConfig.from_pretrained(checkpoint_path, trust_remote_code=True)

    # Fix vocab_size mismatch: tokens were added during training but config wasn't updated
    if model_config.vocab_size != len(tokenizer):
        print(f"  Fixing vocab_size: config has {model_config.vocab_size}, tokenizer has {len(tokenizer)}")
        model_config.vocab_size = len(tokenizer)

    model = AutoModelForSequenceClassification.from_pretrained(
        checkpoint_path,
        config=model_config,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True,
        torch_dtype=torch.bfloat16,
    )
    return model


def load_model_qlora(checkpoint_path, model_name_or_path, tokenizer):
    """Load a QLoRA model: base model in 4-bit + LoRA adapters from checkpoint."""
    from peft import PeftModel

    print(f"Loading QLoRA model: base={model_name_or_path}, adapters={checkpoint_path}")

    # Must match training BitsAndBytesConfig exactly (defaults: fp4, float32, no double quant)
    bnb_config = BitsAndBytesConfig(load_in_4bit=True)

    # Load base model in 4-bit
    base_model = AutoModelForSequenceClassification.from_pretrained(
        model_name_or_path,
        num_labels=len(LABEL_LIST),
        device_map="auto",
        trust_remote_code=True,
        quantization_config=bnb_config,
    )

    # Resize embeddings BEFORE loading adapter (special tokens were added during training)
    base_model.resize_token_embeddings(len(tokenizer))
    if base_model.config.pad_token_id is None:
        base_model.config.pad_token_id = tokenizer.pad_token_id

    # Load LoRA adapters + classification head from checkpoint
    model = PeftModel.from_pretrained(base_model, checkpoint_path)
    return model


def main():
    parser = argparse.ArgumentParser(description="Unified prediction for full-FT and QLoRA checkpoints")
    parser.add_argument("config", type=str, help="Path to prediction config JSON")
    parser.add_argument("--batch_size", type=int, default=None,
                        help="Override batch size from config")
    args = parser.parse_args()

    # Load config
    with open(args.config) as f:
        cfg = json.load(f)

    transform_method = cfg["transform_method"]
    move_entities_to_start = cfg.get("move_entities_to_start", False)
    label_column_name = cfg.get("label_column_name", "type")
    max_len = cfg.get("max_len", 512)
    batch_size = args.batch_size or cfg.get("per_device_eval_batch_size", 16)
    model_name_or_path = cfg["model_name_or_path"]
    use_prompt_format = cfg.get("use_prompt_format", False)

    # Set random seed if present in config
    random_seed = cfg.get("random_seed")
    if random_seed is not None:
        torch.manual_seed(random_seed)
        np.random.seed(random_seed)

    # checkpoint_dir is the full path to the checkpoint (e.g. .../split_0/checkpoint-best)
    checkpoint_path = cfg["checkpoint_dir"]
    output_dir = cfg["output_dir"]

    if not os.path.isdir(checkpoint_path):
        print(f"ERROR: Checkpoint not found at {checkpoint_path}")
        sys.exit(1)

    # Checkpoint type from config; auto-detect as sanity check
    use_qlora = cfg.get("use_qlora", False)
    detected_type = detect_checkpoint_type(checkpoint_path)
    if detected_type is not None:
        detected_qlora = (detected_type == "qlora")
        if detected_qlora != use_qlora:
            print(f"  WARNING: Config has use_qlora={use_qlora} but checkpoint looks like {detected_type}. Using config value.")

    print("=" * 60)
    print("Unified Prediction (device_map='auto')")
    print(f"  Config:          {args.config}")
    print(f"  Checkpoint:      {checkpoint_path}")
    print(f"  Checkpoint type: {'QLoRA' if use_qlora else 'Full fine-tuned'}")
    print(f"  Base model:      {model_name_or_path}")
    print(f"  Prompt format:   {use_prompt_format}")
    print(f"  Batch size:      {batch_size}")
    print(f"  Output dir:      {output_dir}")
    print("=" * 60)

    # --- Load tokenizer from checkpoint ---
    print("\nLoading tokenizer from checkpoint...")
    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    print(f"  Tokenizer vocab size: {len(tokenizer)}")

    # --- Load model ---
    if use_qlora:
        model = load_model_qlora(checkpoint_path, model_name_or_path, tokenizer)
    else:
        model = load_model_full_ft(checkpoint_path, tokenizer)

    if model.config.pad_token_id is None:
        model.config.pad_token_id = tokenizer.pad_token_id
    model.eval()

    total_params = sum(p.numel() for p in model.parameters())
    print(f"  Model params: {total_params:,}")

    input_device = get_input_device(model)
    print(f"  Input device: {input_device}")

    os.makedirs(output_dir, exist_ok=True)

    # --- Load test data and auto-detect labels ---
    print(f"\nProcessing test data: {cfg['testing_dataframes']}")
    df = get_dataframes(cfg["testing_dataframes"])

    has_labels = label_column_name in df.columns
    if has_labels:
        print(f"  Labels detected (column: '{label_column_name}') — will compute metrics")
        df = process_data(df, transform_method, move_entities_to_start, label_column_name, mode='train', use_prompt_format=use_prompt_format)
    else:
        print(f"  No labels (column '{label_column_name}' not found) — prediction only")
        df = process_data(df, transform_method, move_entities_to_start, label_column_name, mode='test', use_prompt_format=use_prompt_format)

    dataset = SentenceDataset(
        sentences=df.text.to_numpy(),
        labels=df.label.to_numpy(),
        tokenizer=tokenizer,
        max_len=max_len,
    )
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False,
                            num_workers=cfg.get("dataloader_num_workers", 4))

    print(f"  Samples: {len(dataset)}, Batches: {len(dataloader)}")
    print("  Running predictions...")
    logits = run_predictions(model, dataloader, input_device, desc="Predict")

    # --- Save predictions ---
    with open(os.path.join(output_dir, 'predictions_prob.txt'), 'w') as f:
        f.writelines('\n'.join(['\t'.join(map(str, x)) for x in logits]) + '\n')

    preds = np.argmax(logits, axis=1)
    predicted_class = [LABEL_DICT[cl] for cl in preds]
    df["predicted_class"] = predicted_class
    df.to_csv(os.path.join(output_dir, "predictions.csv"))
    print(f"  Saved: predictions.csv, predictions_prob.txt")

    # --- Compute metrics if labels were present ---
    all_results = {"n_samples": len(dataset)}
    if has_labels:
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support

        true_labels = df.label.to_numpy()
        accuracy = accuracy_score(true_labels, preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            true_labels, preds,
            labels=list(range(1, len(LABEL_DICT))),
            average='micro', zero_division=0,
        )
        all_results["accuracy"] = accuracy
        all_results["precision"] = precision
        all_results["recall"] = recall
        all_results["f1"] = f1
        print(f"  Metrics: accuracy={accuracy:.4f}, precision={precision:.4f}, "
              f"recall={recall:.4f}, f1={f1:.4f}")

    with open(os.path.join(output_dir, 'all_results.json'), 'w') as f:
        json.dump(all_results, f, indent=4)

    print(f"\nDone. All outputs saved to: {output_dir}")


if __name__ == "__main__":
    main()
